{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "import sys\n",
    "sys.path.append('/Users/katiehuang/Documents/metis/projects/onl_ds5_project_2/py')\n",
    "from get_book import *\n",
    "import importlib\n",
    "from fictiondb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create url list (for 12 pages)\n",
    "listopia_base_url=\"https://www.goodreads.com/list/show/4271.I_Only_Watched_the_Movie_\"\n",
    "listopia_url_list=[listopia_base_url]\n",
    "for i in range(2,13):\n",
    "    url = listopia_base_url + '?page=' + str(i)\n",
    "    listopia_url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup_list (12 soup for 12 pages)\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "# listopia_soup_list=[]\n",
    "# for url in listopia_url_list:\n",
    "#     listopia_soup_list.append(get_soup(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listopia_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-85da6b84d2f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbook_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'itemprop'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'heading'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbook_title_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbook_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "book_title_list=[]\n",
    "book_url_list=[]\n",
    "base_book_url=\"https://www.goodreads.com/\"\n",
    "\n",
    "for page_url in listopia_url_list:\n",
    "    soup = get_soup(page_url)\n",
    "    table= soup.find('table', class_=\"tableList js-dataTooltip\")\n",
    "    rows = [row for row in table.find_all('tr')]\n",
    "\n",
    "    for i in range(100):\n",
    "        book_title = rows[i].find('span', attrs={'itemprop':'name','role':'heading'}).text.split('(')[0].strip(' ')\n",
    "        book_title_list.append(book_title)\n",
    "        book_url = rows[i].find_all('a')[0].get('href')\n",
    "        book_url_list.append(base_book_url+book_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of book+link\n",
    "book_link_df = pd.DataFrame(list(zip(book_title_list,book_url_list)),\\\n",
    "            columns=['book','link_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>link_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>https://www.goodreads.com//book/show/186190.Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Devil Wears Prada</td>\n",
       "      <td>https://www.goodreads.com//book/show/5139.The_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jurassic Park</td>\n",
       "      <td>https://www.goodreads.com//book/show/6424171-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>https://www.goodreads.com//book/show/525995.Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mary Poppins</td>\n",
       "      <td>https://www.goodreads.com//book/show/152380.Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>Betrayal: The Crisis in the Catholic Church</td>\n",
       "      <td>https://www.goodreads.com//book/show/947265.Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>The Private Lives of Pippa Lee</td>\n",
       "      <td>https://www.goodreads.com//book/show/2116927.T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>The Handmaid's Tale</td>\n",
       "      <td>https://www.goodreads.com//book/show/38447.The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>The High Window</td>\n",
       "      <td>https://www.goodreads.com//book/show/2049.The_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>Exodus</td>\n",
       "      <td>https://www.goodreads.com//book/show/42697.Exodus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1122 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book  \\\n",
       "0                                    Forrest Gump   \n",
       "1                           The Devil Wears Prada   \n",
       "2                                   Jurassic Park   \n",
       "3                                         Jumanji   \n",
       "4                                    Mary Poppins   \n",
       "...                                           ...   \n",
       "1117  Betrayal: The Crisis in the Catholic Church   \n",
       "1118               The Private Lives of Pippa Lee   \n",
       "1119                          The Handmaid's Tale   \n",
       "1120                              The High Window   \n",
       "1121                                       Exodus   \n",
       "\n",
       "                                                 link_b  \n",
       "0     https://www.goodreads.com//book/show/186190.Fo...  \n",
       "1     https://www.goodreads.com//book/show/5139.The_...  \n",
       "2     https://www.goodreads.com//book/show/6424171-j...  \n",
       "3     https://www.goodreads.com//book/show/525995.Ju...  \n",
       "4     https://www.goodreads.com//book/show/152380.Ma...  \n",
       "...                                                 ...  \n",
       "1117  https://www.goodreads.com//book/show/947265.Be...  \n",
       "1118  https://www.goodreads.com//book/show/2116927.T...  \n",
       "1119  https://www.goodreads.com//book/show/38447.The...  \n",
       "1120  https://www.goodreads.com//book/show/2049.The_...  \n",
       "1121  https://www.goodreads.com//book/show/42697.Exodus  \n",
       "\n",
       "[1122 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_link_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dump/complete_data_cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2ed0803dbbf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load movie_df and complete_data_cleaned to find movie still needs book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmovie_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dump/movie_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dump/complete_data_cleaned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dump/complete_data_cleaned'"
     ]
    }
   ],
   "source": [
    "# Load movie_df and complete_data_cleaned to find movie still needs book\n",
    "movie_df = pd.read_pickle('../dump/movie_data')\n",
    "all_df = pd.read_pickle('../dump/complete_data_cleaned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = list(movie_df.movie_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = set(movie_list).intersection(set(book_title_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find url for each book from the list (I Saw the Movie & Read the Book) webpages\n",
    "# Try first page (listopia_base_url)\n",
    "\n",
    "base_book_url=\"https://www.goodreads.com/\"\n",
    "book_url_list=[]\n",
    "\n",
    "soup = listopia_soup_list[1]\n",
    "\n",
    "table= soup.find('table', class_=\"tableList js-dataTooltip\")\n",
    "rows = [row for row in table.find_all('tr')]\n",
    "\n",
    "for i in range(100):\n",
    "    book_url = rows[i].find_all('a')[0].get('href')\n",
    "    book_url_list.append(base_book_url+book_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(listopia_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forrest Gump'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('span', attrs={'itemprop':'name','role':'heading'}).text.split('(')[0].strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello dlkjf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hello dlkjf'\n",
    "s.split('(')[0].strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-8f619c128401>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-8f619c128401>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <span itemprop=\"name\" role=\"heading\" aria-level=\"4\">The Help</span>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<span itemprop=\"name\" role=\"heading\" aria-level=\"4\">The Help</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find url for each book from the list (I Saw the Movie & Read the Book) webpages\n",
    "# Create a list of url for 1800+ books\n",
    "\n",
    "book_url_list=[]\n",
    "base_book_url=\"https://www.goodreads.com/\"\n",
    "\n",
    "for soup in listopia_soup_list[:-1]:\n",
    "    table= soup.find('table', class_=\"tableList js-dataTooltip\")\n",
    "    rows = [row for row in table.find_all('tr')]\n",
    "\n",
    "    for i in range(100):\n",
    "        book_url = rows[i].find_all('a')[0].get('href')\n",
    "        book_url_list.append(base_book_url+book_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup_list for the books\n",
    "\n",
    "book_soup_list=[]\n",
    "for book_url in book_url_list[0:201]:\n",
    "    book_soup = get_soup(book_url)\n",
    "    book_soup_list.append(book_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_url in book_url_list[201:901]:\n",
    "    book_soup = get_soup(book_url)\n",
    "    book_soup_list.append(book_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_url in book_url_list[901:]:\n",
    "    book_soup = get_soup(book_url)\n",
    "    book_soup_list.append(book_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(book_soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book_soup_list.txt','w') as f:\n",
    "    f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-159415ee1006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook_soup_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dump/book_soup_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "book_soup_list.to_pickle('../dump/book_soup_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump book_soup_list to pickle\n",
    "import pickle\n",
    "file_name = 'book_soup_list.pkl'\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(book_soup_list, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28672\n",
      "36864\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import resource\n",
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "max_rec = 0x9000\n",
    "sys.setrecursionlimit(max_rec)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book_soup_list.pkl', 'wb') as handle:\n",
    "    pickle.dump(book_soup_list, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get movie data from search result pages\n",
    "def get_bookdata(book): # book = soup in book_soup_list\n",
    "    \n",
    "    headers=['book_title', 'author', 'rating_value', 'rating_count', 'review_count', 'page', 'year']\n",
    "    \n",
    "    # Find book title, author, rating_value, rating_count, review_count, page\n",
    "    title = book.find('h1').text.strip('\\n').lstrip()\n",
    "    author = book.find('a', class_=\"authorName\").text\n",
    "    rating_value = float(book.find('span', attrs={'itemprop':'ratingValue'}).text.strip('\\n').lstrip())\n",
    "    rating_count = int(book.find('meta', attrs={'itemprop':'ratingCount'}).get('content'))\n",
    "    review_count = int(book.find('meta', attrs={'itemprop':'reviewCount'}).get('content'))\n",
    "    page = int(book.find('span', attrs={'itemprop':'numberOfPages'}).text.strip(' pages'))\n",
    "\n",
    "    # Find year first published\n",
    "    for div in book.find_all('div', attrs={'id':'details'}):\n",
    "        for row in div.find_all('nobr', class_='greyText'):\n",
    "            year = row.text\n",
    "            year = int(''.join(i for i in year if i.isdigit())[-4:])\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Find earliest year\n",
    "    string = div.find_all('div', class_='row')[1].text\n",
    "    str_clean = string.replace('(',' ').replace(')',' ').split()\n",
    "    year_list = [int(s) for s in str_clean if s.isdigit()]\n",
    "    year = min(year_list)\n",
    "    \n",
    "    \n",
    "    book_dict = dict(zip(headers, [title,\n",
    "                                   author,\n",
    "                                   rating_value,\n",
    "                                   rating_count,\n",
    "                                   review_count,\n",
    "                                   page,\n",
    "                                   year]))\n",
    "    \n",
    "    return book_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get book data from the book page\n",
    "# Try fixing missing value problem\n",
    "# Try fixing first published year problem (if there's only first published year)\n",
    "\n",
    "\n",
    "def get_bookdata(book): # book = soup in book_soup_list\n",
    "    \n",
    "    headers=['book_title', 'author', 'rating_value', 'rating_count', 'review_count', 'page', 'year']\n",
    "    \n",
    "    # Assign default value\n",
    "    title, author, rating_value, rating_count, review_count, page, year = \\\n",
    "    np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    # Find book title, author, rating_value, rating_count, review_count, page\n",
    "    title = book.find('h1').text.strip('\\n').lstrip()\n",
    "    author = book.find('a', class_=\"authorName\").text\n",
    "    rating_value = float(book.find('span', attrs={'itemprop':'ratingValue'}).text.strip('\\n').lstrip())\n",
    "    rating_count = int(book.find('meta', attrs={'itemprop':'ratingCount'}).get('content'))\n",
    "    review_count = int(book.find('meta', attrs={'itemprop':'reviewCount'}).get('content'))\n",
    "    if book.find('span', attrs={'itemprop':'numberOfPages'}) is not None:\n",
    "        page = int(book.find('span', attrs={'itemprop':'numberOfPages'}).text.strip(' pages'))\n",
    "\n",
    "    # Find year first published\n",
    "    for div in book.find_all('div', attrs={'id':'details'}):\n",
    "        for row in div.find_all('nobr', class_='greyText'):\n",
    "            year = row.text\n",
    "            year = int(''.join(i for i in year if i.isdigit())[-4:])\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Find earliest year\n",
    "    if div.find_all('div', class_='row') != []:\n",
    "        string = div.find_all('div', class_='row')[-1].text\n",
    "        str_clean = string.replace('(',' ').replace(')',' ').split()\n",
    "        year_list = [int(s) for s in str_clean if s.isdigit()]\n",
    "        if year_list != []:\n",
    "            year = min(year_list)\n",
    "    \n",
    "    \n",
    "    book_dict = dict(zip(headers, [title,\n",
    "                                   author,\n",
    "                                   rating_value,\n",
    "                                   rating_count,\n",
    "                                   review_count,\n",
    "                                   page,\n",
    "                                   year]))\n",
    "    \n",
    "    return book_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Harry Potter #1\n",
    "hp_url = book_url_list[0]\n",
    "hp = book_soup_list[0]\n",
    "get_bookdata(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of book_dict\n",
    "book_data=[]\n",
    "for book_soup in book_soup_list:\n",
    "    book = get_bookdata(book_soup)\n",
    "    book_data.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bookdata(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data=[]\n",
    "for book_soup in book_soup_list[:8]:\n",
    "    book = get_bookdata(book_soup)\n",
    "    book_data.append(book)\n",
    "book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"https://www.goodreads.com/list/show/2451.I_Saw_the_Movie_Read_the_Book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table= soup.find('table', class_=\"tableList js-dataTooltip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [row for row in table.find_all('tr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[1].find_all('a')[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[1].find_all('a')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_url=\"https://www.goodreads.com/book/show/3.Harry_Potter_and_the_Sorcerer_s_Stone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_soup=get_soup(hp_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = hp_soup.find('h1').text.strip('\\n').lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = hp_soup.find('a', class_=\"authorName\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_value = float(hp_soup.find('span', attrs={'itemprop':'ratingValue'}).text.strip('\\n').lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = int(hp_soup.find('meta', attrs={'itemprop':'ratingCount'}).get('content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = int(hp_soup.find('span', attrs={'itemprop':'numberOfPages'}).text.strip(' pages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in hp_soup.find_all('div', attrs={'id':'details'}):\n",
    "#     print(div)\n",
    "    for row in div.find_all('div', class_='row'):\n",
    "        print('hi')\n",
    "        print(row)\n",
    "        row.find_all('nobr', class_='greyText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in hp_soup.find_all('div', attrs={'id':'details'}):\n",
    "#     print(div)\n",
    "    for row in div.find_all('div', class_='row'):\n",
    "        print('hi')\n",
    "        print(row)\n",
    "        row.find_all('div', class_='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div.find_all('div', class_='row')[1].text.partition('\\n')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in hp_soup.find_all('div', attrs={'id':'details'}):\n",
    "    for row in div.find_all('nobr', class_='greyText'):\n",
    "        year = row.text\n",
    "        year = int(''.join(i for i in year if i.isdigit())[-4:])\n",
    "    if year is None:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = div.find_all('div', class_='row')[1].text.partition('\\n')[2]\n",
    "str = div.find_all('div', class_='row')[1].text\n",
    "str_clean = str.replace('(',' ').replace(')',' ').split()\n",
    "year_list = [int(s) for s in str_clean if s.isdigit()]\n",
    "year = min(year_list)\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div.find_all('div', class_='row')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str = str.replace('(',' ').replace(')',' ').split()\n",
    "[int(s) for s in str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = \"(first published June 26th 1997)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.strip('(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.strip('(').strip(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.strip('(').strip(')').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
